{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DATA 612 Project 6:  Hands on with AWS**\n",
        "\n",
        "**Gullit Navarrete**\n",
        "\n",
        "**7/13/25**"
      ],
      "metadata": {
        "id": "UAMVwOJTbCo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "Taking the recommender system I built on MovieLens 1M ratings and deploying it end-to-end in AWS, this project moves from a local notebook into a secure, serverless cloud setup. First, I demonstrate long term storage by uploading the data to S3. Next, I deploy the recommendation code as an AWS Lambda function which was invoked straight from Colab, to pull data from S3, run the scripts, and write outputs. Finally, I lock everything down in a VPC so that only that Lambda can securely access the bucket.\n",
        "\n",
        "\n",
        "**Data Import: via Download and Unpacking**\n",
        "\n",
        "From my previous project assignments, I'll still choose the MovieLens Movie Ratings Dataset, only with the MovieLens 1M ratings dataset rather than the previous 100K. I'll code the download and the unpacking of MovieLen's zip file for replication."
      ],
      "metadata": {
        "id": "orA5sdhynZpa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd0BtwGl_OCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88327a35-82f4-40e0-f586-110d862dffb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ml-1m/movies.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Download\n",
        "!wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
        "# Unpack\n",
        "!unzip -q ml-1m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Long Term Storage:**\n",
        "\n",
        "In order to utilize some sort of long term storage, I'll upload the MovieLens files into a S3 bucket (which I called data612project6movielens1m) by installing and importing the Python AWS SDK (boto3), then use my S3 credentials, target region, and bucket name. It builds an S3 client with those values and then loops over the three data Movielens files (ratings.dat, users.dat, movies.dat). The result confirms that the raw data now lives in S3 as the project's long term storage."
      ],
      "metadata": {
        "id": "ie9AyVVsTGel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet boto3\n",
        "import boto3\n",
        "\n",
        "AWS_KEY = \"AKIA4MPJOCJWHHBBLL4X\" # Access key ID\n",
        "AWS_SECRET = \"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\" # Secret access key\n",
        "REGION = \"us-east-2\"\n",
        "BUCKET = \"data612project6movielens1m\"\n",
        "\n",
        "s3 = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id = AWS_KEY,\n",
        "    aws_secret_access_key = AWS_SECRET,\n",
        "    region_name = REGION\n",
        ")\n",
        "\n",
        "for fname in [\"ratings.dat\", \"users.dat\", \"movies.dat\"]:\n",
        "    local_path = f\"ml-1m/{fname}\"\n",
        "    print(f\"Uploading {local_path} → s3://{BUCKET}/{fname}\")\n",
        "    s3.upload_file(local_path, BUCKET, fname)\n",
        "\n",
        "print(\"This shows that MovieLens 1M Ratings were uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XYfBYPBEQB",
        "outputId": "95952a10-833e-4c79-e202-aa312e09322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading ml-1m/ratings.dat → s3://data612project6movielens1m/ratings.dat\n",
            "Uploading ml-1m/users.dat → s3://data612project6movielens1m/users.dat\n",
            "Uploading ml-1m/movies.dat → s3://data612project6movielens1m/movies.dat\n",
            "This shows that MovieLens 1M Ratings were uploaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Service**\n",
        "\n",
        "For the sake of keeping this project within Google Colab and for its demonstration purposes, first I had to create an IAM role trusted by AWS Lambda and generate an IAM user's programmatic access keys with S3 and Lambda permissions. I then built a small-scale Lambda handler, deployed it to AWS, and invoked it all without leaving the Google Colab notebook. First, I wrote out the handler.py file (which returns a dummy top 3 recommendation for any user ID passed to it), then zipped that file into lambda_package.zip. Next, using the Boto3 Lambda client, I checked whether a function named “RecommenderLambda” already existed, if so, I updated its code. If not, I am simply creating it from scratch, pointing it at my same IAM role. Finally, we invoked the function with a test payload ({\"user_id\": 1}) and printed the JSON-encoded response. The result demonstrated end-to-end use of AWS Lambda as a compute service directly from the Google Colab notebook in code authoring, packaging, deployment, and execution, all via Python."
      ],
      "metadata": {
        "id": "UyjOAyWpnzMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, boto3, zipfile, os\n",
        "\n",
        "AWS_KEY = \"AKIA4MPJOCJWHHBBLL4X\"\n",
        "AWS_SECRET = \"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\"\n",
        "REGION = \"us-east-2\"\n",
        "LAMBDA_ROLE_ARN  = \"arn:aws:iam::851429954156:role/project6roleread612\"\n",
        "FUNCTION_NAME = \"RecommenderLambda\"\n",
        "\n",
        "lambda_client = boto3.client(\n",
        "    \"lambda\",\n",
        "    aws_access_key_id     = AWS_KEY,\n",
        "    aws_secret_access_key = AWS_SECRET,\n",
        "    region_name           = REGION,\n",
        ")\n",
        "\n",
        "handler_code = '''\\\n",
        "import json\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    user = event.get(\"user_id\", \"<none>\")\n",
        "    # dummy recommendations\n",
        "    return {\n",
        "        \"statusCode\": 200,\n",
        "        \"body\": json.dumps({\n",
        "            \"user\": user,\n",
        "            \"recommendations\": [42, 317, 108]\n",
        "        })\n",
        "    }\n",
        "'''\n",
        "with open(\"handler.py\",\"w\") as f:\n",
        "    f.write(handler_code)\n",
        "\n",
        "with zipfile.ZipFile(\"lambda_package.zip\",\"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(\"handler.py\")\n",
        "\n",
        "# # Lambda update\n",
        "try:\n",
        "    lambda_client.get_function(FunctionName=FUNCTION_NAME)\n",
        "    print(\"➤ Updating existing Lambda…\")\n",
        "    lambda_client.update_function_code(\n",
        "        FunctionName=FUNCTION_NAME,\n",
        "        ZipFile=open(\"lambda_package.zip\",\"rb\").read()\n",
        "    )\n",
        "except lambda_client.exceptions.ResourceNotFoundException:\n",
        "    print(\"➤ Creating new Lambda…\")\n",
        "    lambda_client.create_function(\n",
        "        FunctionName=FUNCTION_NAME,\n",
        "        Runtime=\"python3.9\",\n",
        "        Role=LAMBDA_ROLE_ARN,\n",
        "        Handler=\"handler.lambda_handler\",\n",
        "        Code={\"ZipFile\": open(\"lambda_package.zip\",\"rb\").read()},\n",
        "        Timeout=30,\n",
        "        MemorySize=128,\n",
        "    )\n",
        "\n",
        "# Invoke\n",
        "response = lambda_client.invoke(\n",
        "    FunctionName=FUNCTION_NAME,\n",
        "    InvocationType=\"RequestResponse\",\n",
        "    Payload=json.dumps({\"user_id\": 1})\n",
        ")\n",
        "result = json.load(response[\"Payload\"])\n",
        "print(\"Lambda returned:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1jBz8Llnzkr",
        "outputId": "3766c061-626c-437a-c134-24d66704ca03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➤ Updating existing Lambda…\n",
            "Lambda returned: {'statusCode': 200, 'body': '{\"user\": 1, \"recommendations\": [42, 317, 108]}'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VPC**\n",
        "\n",
        "A Virtual Private Cloud, or a VPC, is how I can isolate the network inside AWS, basically giving me complete control over IP ranges, subnets, and routing so that only your resources can reach each other securely. By placing a S3 gateway endpoint in the VPC, it prevents public Internet access to the MovieLens files, because only clients inside your network can interact, see, and use them. For example, Chase as a banking finance company configures S3 gateway endpoints inside its VPC so that all cardholder transaction logs flow directly from their compute instances to S3 without ever reaching the public internet. To enable this, I went into the AWS Console's VPC service, clicked “Create VPC”, turned on DNS resolution, created public and private subnets, attached an Internet Gateway, and added the S3 endpoint for all of the bucket traffic stays within my new VPC.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Throughout my hands on approach with AWS, I demonstrated how to extend a traditional recommender system pipeline into a cloud environment by first addressing the long term storage requirement. I chose Amazon S3 to house the MovieLens 1M dataset because it provides virtually unlimited, durable object storage and integrates seamlessly with the AWS ecosystem. Using the AWS CLI and then boto3 in Google Colab, I automated the download, extraction, and upload of the ratings, movies, and users files into a private S3 bucket (data612project6movielens1m), satisfying the “file storage” deliverable.\n",
        "\n",
        "Next, to fulfill the compute requirement, I opted for AWS Lambda invoked directly from within Google Colab. Outside of Colab, I created an IAM role (named project6roleread612) with the necessary S3 read permissions and attached the AWSLambdaBasicExecutionRole policy. Back in Colab, I wrote a simple Python handler, packaged it into a ZIP, and used boto3 to create (or update) the RecommenderLambda function. Finally, I invoked the function with a sample payload, receiving a dummy list of recommendations, done all without provisioning or managing servers.\n",
        "\n",
        "For network isolation, I created a dedicated VPC (data612-project6-vpc) in the AWS console. By selecting auto generating public and private subnets across two AZs, enabling DNS resolution, and adding an S3 gateway endpoint, I making sure that all data traffic between Lambda and S3 remains within the AWS network fulfilling the VPC deliverable and preventing any exposure to the public Internet.\n",
        "\n",
        "I chose S3 and Lambda primarily for their ease of use on the AWS Free Tier and tight integration with Python environments like Google Colab where I do my assignments and Projects on. Advantages include S3's scalability, low operational overhead, and Lambda's zero server maintenance and automated scaling. However disadvantages include S3's potential costs at high egress volumes and Lambda's cold start latency and timeout limits. Nevertheless, this combination offers a lightweight, cost-effective way to prototype a cloud-native recommender pipeline while meeting all three deliverables.\n"
      ],
      "metadata": {
        "id": "HPk0Wf9U0sRz"
      }
    }
  ]
}